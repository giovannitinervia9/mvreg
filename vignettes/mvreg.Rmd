---
title: "mvreg"
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
vignette: >
  %\VignetteIndexEntry{mvreg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.width=12/2.5, fig.height=8/2.5)
```

```{r setup, include = F}
library(mvreg)
suppressMessages(require(dplyr))
suppressMessages(require(ggplot2))
suppressMessages(require(ggthemes))
```

# Why `mvreg`? The heteroscedasticity problem

`mvreg` is a package that allows users to fit linear models in which the assumption of homoscedasticity can be relaxed.

It is well known that in the standard linear regression model one of the assumptions is that the variance of the residuals is constant for all the observations, yet in real applications it is not rare to encounter cases in which this assumptions doesn't hold.

In these cases the Ordinary Least Squares estimator for parameters' vector $\boldsymbol{\beta}$ is unbiased, but the standard errors we get from `lm()` are not consistent anymore.

This problem can be handled in several ways, like sandwich variance estimation provided by `sandwich` package or via Weighted Least Squares estimation, in which one can estimate $\boldsymbol{\beta}$ via $\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$, providing a weights matrix $\mathbf{W}$ that should be a diagonal matrix with diagonal elements $w_{ii} = \dfrac{1}{\sigma^2_{i}}$.

# What does `mvreg` do?
The approach that `mvreg` follows is to fit a model in which expected value and variance are jointly modelled as functions of linear predictors, which can have different structures for the two components. 

Formally, the model is specified as $$Y_i \sim \mathcal{N}\left(\mu_i = \mathbf{x}_i'\boldsymbol{\beta}, \sigma^2_i = \exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}\right)$$
where $\mathbf{x}_i$ is the $i$-th row vector of a design matrix $\mathbf{X}$ and $\boldsymbol{\beta}$ is the vector of parameters for the mean component, while $\mathbf{z}_i$ is the $i$-th row vector of a design matrix $\mathbf{Z}$ and $\boldsymbol{\tau}$ is the vector of parameters for the variance component.

In general, $\mathbf{X}$ and $\mathbf{Z}$ can share the same columns, can have totally different columns or the columns of $\mathbf{Z}$ can be a subset of the columns of $\mathbf{X}$ and viceversa.

The variance is linked to the linear predictor $\mathbf{z}_i'\boldsymbol{\tau}$ via a $\log$-link, so that 

$$\log{(\sigma^2_i)} = \mathbf{z}_i'\boldsymbol{\tau}$$

Model parameters are estimated using the maximum likelihood method, but before delving into technical details let's see how `mvreg` works.

## `mvreg()` function, a `lm`-like function for heteroscedastic regression.

The main function that a generic user of `mvreg` should use is the `mvreg()` function. 
This function allows to fit a heroscedastic linear model in which two different formulas can be specified, one for mean component and the other one for variance component.

Let's see some examples of different case uses.

For the first example we'll use the `cars`. This dataset contains only two variables, `speed` and `dist`, so that it's natural to consider a model that explains how stopping distance is affected by speed. Let's visualize the data.

```{r, echo = F, warning = F, message = F}
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point(shape = 1) + 
  geom_smooth(linewidth = 0.5, method = "lm") +
  theme_few()
```

It is clearly evident that stopping distance increases as speed increases, and a keen eye might notice that for low values of speed the stopping distance tends to assume values closer to the regression line, while, as speed increases, the points tend to be more dispersed.

Let's fit a classic linear model and check the relationship between the square root of the absolute value of residuals and the fitted values.

```{r}
mod.lm <- lm(dist ~ speed, data = cars)
```

```{r, echo = F, warning = F, message = F}
# cars |>
#   mutate(residuals = scale(mod.lm$residuals), 
#          fitted.values = mod.lm$fitted.values) |> 
#   ggplot(aes(x = fitted.values, y = sqrt(abs(residuals)))) + 
#   geom_point(shape = 1) + 
#   geom_smooth(color = "red", linewidth = 0.5) + 
#   labs(y = expression(sqrt(abs(scaled~residuals)))) +
#   labs(x = "fitted values") + 
#   theme_few()

plot(mod.lm, 3)
```

The graph seems to confirm the initial evidence with respect to the possibility of observing heteroscedasticity, so we could try to account for it via `mvreg`.

`mvreg()` function works just like `lm()` function. We can specify two different formulas for the mean component, which is called `formula.mu`, and for the variance component, which is called `formula.s2`.

```{r}
mod.mvreg <- mvreg(formula.mu = dist ~ speed, 
                   formula.s2 = ~speed, data = cars)
```

`mvreg()` returns an object of class `mvreg`, for which various generic methods are implemented.
Let's see the `summary()` of the model.

```{r}
summary(mod.mvreg)
```
As it can be seen, the output of `summary()` returns two main tables. In the first table estimates, standard errors and the results of Wald tests are reported for the mean component, whereas the second table reports same information for the coefficient of the `log(variance)` component. These tables have the same structure of a classical output of `lm` objects, with the main difference that Wald tests are computed assuming asymptotic normal distribution under the null hypothesis that the coefficients are equal to zero.

It can be seen that the coefficient relative to the variance component for the variable `speed` is significantly different from zero and it's positive, suggesting a positive linear relation between the logarithm of the variance and speed.

## How to handle `formula.s2`
The argument `formula.s2` is optional. If it is not specified it is assumed that the structure of linear predictor is the same both in mean and variance component.

For example, if we write

```{r}
mvreg(dist ~ speed, data = cars)
```

we get the same model we fitted in the previous section.

If we do not want the formulas to be equal, it is necessary to specify both of them, as in 

```{r}
mvreg(dist ~ speed + I(speed^2),
      ~speed, data = cars)
```
`formula.s2` can be both a two-side formula or a only right-side formula. `mvreg()` essentially ignores the left side of `formula.s2`, as it is needed only to build the design matrix for variance component.






