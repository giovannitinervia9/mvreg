---
title: "mvreg"
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
vignette: >
  %\VignetteIndexEntry{mvreg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.width = 12 / 2.5, fig.height = 8 / 2.5)
```

```{r setup, include = F}
library(mvreg)
suppressMessages(require(dplyr))
suppressMessages(require(ggplot2))
suppressMessages(require(ggthemes))
```

# Why `mvreg`? The heteroscedasticity problem

`mvreg` is a package that allows users to fit linear models in which the assumption of homoscedasticity can be relaxed.

It is well known that in the standard linear regression model one of the assumptions is that the variance of the residuals is constant for all the observation. However, in real-word applications it is not uncommon to encounter cases where this assumption does not hold.

In such cases, the Ordinary Least Squares (OLS) estimator for the parameter vector $\boldsymbol{\beta}$ remains unbiased, but the standard errors produced by lm() are no longer consistent.

This problem can be addressed in several ways, such as using sandwich variance estimation, as provided by the `sandwich` package, or by employing Weighted Least Squares (WLS) estimation. In WLS, the $\boldsymbol{\beta}$ parameter is estimated as $\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$, where $\mathbf{W}$ is a diagonal weight matrix with diagonal elements $w_{ii} = \dfrac{1}{\sigma^2_{i}}$.

# What does `mvreg` do?
The approach that `mvreg` follows is to fit a model in which expected value and variance are jointly modeled as functions of linear predictors, which can have different structures for the two components. 

Formally, the model is specified as $$Y_i|\mathbf{x}_i, \mathbf{z}_i \sim \mathcal{N}\left(\mu_i = \mathbf{x}_i'\boldsymbol{\beta}, \sigma^2_i = \exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}\right)$$
where $\mathbf{x}_i$ is the $i$-th row vector of a $n \times k$ design matrix $\mathbf{X}$ and $\boldsymbol{\beta}$ is the vector of parameters for the mean component, while $\mathbf{z}_i$ is the $i$-th row vector of a $n \times p$ design matrix $\mathbf{Z}$ and $\boldsymbol{\tau}$ is the vector of parameters for the variance component.

In general, $\mathbf{X}$ and $\mathbf{Z}$ can share the same columns, have totally different columns or the columns of $\mathbf{Z}$ can be a subset of the columns of $\mathbf{X}$ and vice versa.

The variance is linked to the linear predictor $\mathbf{z}_i'\boldsymbol{\tau}$ via a $\log$-link, so that 

$$\log{(\sigma^2_i)} = \mathbf{z}_i'\boldsymbol{\tau}$$

The model parameters are estimated using maximum likelihood method, but before delving into technical details let's see how `mvreg` works.

## `mvreg()`: a `lm`-like function for heteroscedastic regression.

The main function that a typical `mvreg` user will interact with is the `mvreg()` function.
This function fits a heteroscedastic linear model in which two different formulas can be specified: one for the mean component and another for the variance component.

Let's consider the dataset `cars`, which contains only two variables, `speed` and `dist`, making it natural to model how stopping distance is influenced by speed. Let's visualize the data.

```{r, echo = F, warning = F, message = F}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point(shape = 1) +
  geom_smooth(linewidth = 0.5, method = "lm") +
  theme_few()
```

It is clear that stopping distance increases with speed, and one might notice that for lower speeds, stopping distances tend to assume values closer to the regression line, whereas at higher speeds, the points tend to be more dispersed.

Let's fit a classic linear model and check the relationship between the square root of the absolute value of residuals and the fitted values.

```{r}
mod.lm <- lm(dist ~ speed, data = cars)
```

```{r, echo = F, warning = F, message = F}
# cars |>
#   mutate(residuals = scale(mod.lm$residuals),
#          fitted.values = mod.lm$fitted.values) |>
#   ggplot(aes(x = fitted.values, y = sqrt(abs(residuals)))) +
#   geom_point(shape = 1) +
#   geom_smooth(color = "red", linewidth = 0.5) +
#   labs(y = expression(sqrt(abs(scaled~residuals)))) +
#   labs(x = "fitted values") +
#   theme_few()

plot(mod.lm, 3)
```

The graph seems to confirm initial suspicions about heteroscedasticity, so we can attempt to account for it using `mvreg`.

The `mvreg()` function operates similarly to the `lm()` function. We can specify two different formulas: one for the mean component, `formula.mu`, and another for the variance component, `formula.s2`.

```{r}
mod.mvreg <- mvreg(
  formula.mu = dist ~ speed,
  formula.s2 = ~speed, data = cars
)
```

`mvreg()` returns an object of class `mvreg`, for which various generic methods are available.
Let's examine the `summary()` of the model.

```{r}
summary(mod.mvreg)
```
As shown, the output of `summary()` returns two main tables: one for the `mean` component and another for the `log(variance)` component. Both tables include estimates, standard errors, and Wald test results, similar to the classical `lm` output. The key difference is that Wald tests assume an asymptotic normal distribution under the null hypothesis that the coefficients are zero.

The coefficient for the variance component related to `speed` is significantly different from zero and is positive, indicating a positive linear relationship between the logarithm of the variance and speed.

## How to handle `formula.s2`
The `formula.s2` argument is optional. If not provided, it is assumed that the linear predictor structure is the same for both the mean and variance components.

For example, this model:

```{r}
mvreg(dist ~ speed, data = cars)
```

yields the same result as the model fitted in the previous section.

If we want different formulas for the mean and variance components, we must specify both, as shown here: 

```{r}
mvreg(dist ~ speed + I(speed^2),
  ~speed,
  data = cars
)
```
The `formula.s2` can be a two-sided formula or a right-hand-side formula. `mvreg()` essentially ignores the left-hand side of `formula.s2`, as it is only needed to construct the design matrix for the variance component.


# Methods and applications
Let's look at the statistical methods behind `mvreg`.

## Model specification and loglikelihood

As before, the model is specified as

$$Y_i|\mathbf{x}_i, \mathbf{z}_i \sim \mathcal{N}\left(\mu_i = \mathbf{x}_i'\boldsymbol{\beta}, \sigma^2_i = \exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}\right)$$
with probability density function

$$f(y_i|\mathbf{x}_i, \mathbf{z}_i) = \dfrac{1}{\sqrt{2\pi\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}}\exp\left\{ -\dfrac{1}{2}\dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}} \right\}$$

and log-likelihood function


$$ \ell(\boldsymbol{\beta},\boldsymbol{\tau}) = -\dfrac{1}{2}\sum_{i=1}^n \left\{\log(2\pi) + \mathbf{z}_i'\boldsymbol{\tau} + \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}} \right\} $$

The expressions for the first and second derivatives of the log-likelihood are as follows:

$$\dfrac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \dfrac{y_i - \mathbf{x}_i'\boldsymbol{\beta}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}x_{ij}$$
$$\dfrac{\partial^2 \ell}{\partial \beta_j \partial \beta_l} = -\sum_{i=1}^n \dfrac{x_{ij}x_{il}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}$$

$$\dfrac{\partial \ell}{\partial \tau_j} = -\dfrac{1}{2}\sum_{i=1}^n\left\{ 1 - \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}z_{ij}\right\} $$

$$\dfrac{\partial^2 \ell}{\partial \tau_j \partial \tau_l} = -\dfrac{1}{2}\sum_{i=1}^n \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}z_{ij}z_{il} $$

$$\dfrac{\partial^2 \ell}{\partial \beta_j \partial \tau_j} = -\sum_{i=1}^n \dfrac{y_i - \mathbf{x}_i'\boldsymbol{\beta}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}x_{ij}z_{il}$$

With $\mathcal{U}_\boldsymbol{\beta}(\boldsymbol{\beta}, \boldsymbol{\tau})$ and $\mathcal{U}_\boldsymbol{\tau}(\boldsymbol{\beta}, \boldsymbol{\tau})$ we indicate the gradient column vectors with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ respectively.

With $\mathcal{H}_\boldsymbol{\beta}(\boldsymbol{\beta}, \boldsymbol{\tau})$ and $\mathcal{H}_\boldsymbol{\tau}(\boldsymbol{\beta}, \boldsymbol{\tau})$ we indicate the blocks of the hessian matrix with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ respectively.

Similarly, $\mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})$ is the block of cross derivatives with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$.

The full hessian is 

$$\mathcal{H}(\boldsymbol{\beta}, \boldsymbol{\tau}) = 
\begin{bmatrix}
    \mathcal{H}_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\tau}) & \mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau}) \\
    \mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}'(\boldsymbol{\beta}, \boldsymbol{\tau}) & \mathcal{H}_{\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})
\end{bmatrix}$$

$\mathcal{J}(\boldsymbol{\beta}, \boldsymbol{\tau}) = -\mathcal{H}(\boldsymbol{\beta}, \boldsymbol{\tau})$ is the observed Fisher information matrix.

The expected Fisher information matrix is defined as $\mathcal{I}(\boldsymbol{\beta}, \boldsymbol{\tau}) = \mathbb{E}(\mathcal{J}(\boldsymbol{\beta}, \boldsymbol{\tau}))$ and it is used in the computation of variance-covariance matrix of the estimates of parameters.

We have

$$\left\{\mathcal{I}_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\tau})\right\}_{jl} = \sum_{i=1}^n \dfrac{x_{ij}x_{il}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}$$



$$\left\{\mathcal{I}_{\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})\right\}_{jl} = \dfrac{1}{2}\sum_{i=1}^n z_{ij}z_{il}$$
$$\left\{\mathcal{I}_{\boldsymbol{\beta}\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})\right\}_{jl} = 0$$


## Parameters estimation
The vectors of parameters $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ are estimated via maximum likelihood.

A closed form expression for $\hat{\boldsymbol{\beta}}$ is the WLS estimator $\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$, in which $\mathbf{W}$ is a diagonal matrix with $w_{ij} = \dfrac{1}{\exp\left\{\mathbf{z}_i'\hat{\boldsymbol{\tau}}\right\}} \text{ } \forall i = j$.

There is not a closed form expression for $\hat{\boldsymbol{\tau}}$, so Newton-Raphson method is required to estimate it.

### Starting values
Starting values are provided by function `mvreg_start()`, which takes in input the vector of response variable `y`, the design matrices `x` and `z` and the specification of the method to get the first guess of $\hat{\boldsymbol{\tau}}_0$ via the argument `start.s2`.
A starting value of $\hat{\boldsymbol{\beta}}_0$ is computed via classic OLS estimation.
There are three different ways to get a initial guess of $\hat{\boldsymbol{\tau}}_0$.

The default method is provided by `start.s2 = "residuals"`, in which $\hat{\boldsymbol{\beta}}_0$ is used to estimate empirical residuals $\hat{\varepsilon}_i = y_i - \mathbf{x}_i'\hat{\boldsymbol{\beta}}_0$. Then the log of the squares of these residuals are regressed on $\mathbf{Z}$ to find a first guess of $\hat{\boldsymbol{\tau}}_0$. 

A second option is `start.s2 = "gamma"`, in which the squares of the empirical residuals are regressed on $\mathbf{Z}$ via a Gamma glm regression.

The last option is `start.s2 = "zero"`, in which a first guess is simply given by  $\hat{\boldsymbol{\tau}}_0 = (\log\hat{S}^2, 0, 0, \ldots, 0)_p'$.

### Estimation algorithm
The algorithm for fitting the parameters via maximum likelihood is contained in the `mvreg_fit()` function, which takes in input the vector of response variable `y`, the design matrices `x` and `z` and starting values `b0` and `t0` provided by `mvreg_start()`.

At the $k$-th iteration, the algorithm takes values of $\hat{\boldsymbol{\beta}}_{k-1}$ and $\hat{\boldsymbol{\tau}}_{k-1}$ and uses them to compute 

$$\hat{\boldsymbol{\tau}}_k = \hat{\boldsymbol{\tau}}_{k-1} - \mathcal{H}^{-1}_\boldsymbol{\tau}(\hat{\boldsymbol{\beta}}_{k-1}, \hat{\boldsymbol{\tau}}_{k-1}) \mathcal{U}_\boldsymbol{\tau}(\hat{\boldsymbol{\beta}}_{k-1}, \hat{\boldsymbol{\tau}}_{k-1})$$

Then weights $w_i$ are estimated via $w_i = \dfrac{1}{\exp\left\{\mathbf{z}_i'\hat{\boldsymbol{\tau}_{k}}\right\}}$ and used to to compute $$\hat{\boldsymbol{\beta}}_k = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$$

The algorithm continues until $|(\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\tau}})_k - (\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\tau}})_{k-1}| < \boldsymbol{\varepsilon}$

This algorithm is the default choice for `mvreg_fit()`, but an argument `method` is available. If `method = "wls"` the algorithm just described is used, else if `method = "full_nr"` the closed form expression for $\hat{\boldsymbol{\beta}}$ is not used and Newton method is used for parameters of both mean and variance components.

Once a `mvreg` model is fitted, one can access the estimates of parameters via `coef` method, that has a `partition` argument by which one can decide if all coefficients must be returned (default choice, `partition = "all"`) or just the coefficients of mean component (`partition = "mu"`) or variance component (`partition = "s2"`)
```{r}
coef(mod.mvreg)
coef(mod.mvreg, "mu")
coef(mod.mvreg, "s2")
```



## Variance-covariance matrix of estimators

As for asymptotic likelihood theory, we have that $$ (\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})' \rightarrow \mathcal{N}\left[ (\boldsymbol{\beta}, \boldsymbol{\tau})', \mathcal{I}^{-1}(\boldsymbol{\beta}, \boldsymbol{\tau})\right] $$
so an estimate of variance-covariance matrix is given by $\mathcal{I}^{-1}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})$

The variance-covariance matrix is computed by `mvreg_fit()` function, that has an argument `vcov.type`. By default `vcov.type = "expected"`, so the variance-covariance matrix is computed using expected Fisher information matrix $\mathcal{I}^{-1}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})$, but the user can choose to use the observed Fisher information matrix $\mathcal{J}^{-1}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})$.

Once a `mvreg` model is fitted, the variance-covariance matrix can be accessed using function `vcov` 
```{r}
vcov(mod.mvreg)
```

`vcov` method for `mvreg` object has an argument `partition` in which the user can specify which diagonal block of the full matrix is to be returned. The default is `"all"`, but one can also specify `partition = "mu"` or `partition = "s2"`

```{r}
vcov(mod.mvreg, "mu")
vcov(mod.mvreg, "s2")
```




























