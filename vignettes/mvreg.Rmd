---
title: "mvreg"
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
vignette: >
  %\VignetteIndexEntry{mvreg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.width=12/2.5, fig.height=8/2.5)
```

```{r setup, include = F}
library(mvreg)
suppressMessages(require(dplyr))
suppressMessages(require(ggplot2))
suppressMessages(require(ggthemes))
```

# Why `mvreg`? The heteroscedasticity problem

`mvreg` is a package that allows users to fit linear models in which the assumption of homoscedasticity can be relaxed.

It is well known that in the standard linear regression model one of the assumptions is that the variance of the residuals is constant for all the observations, yet in real applications it is not rare to encounter cases in which this assumptions doesn't hold.

In these cases the Ordinary Least Squares estimator for parameters' vector $\boldsymbol{\beta}$ is unbiased, but the standard errors we get from `lm()` are not consistent anymore.

This problem can be handled in several ways, like sandwich variance estimation provided by `sandwich` package or via Weighted Least Squares estimation, in which one can estimate $\boldsymbol{\beta}$ via $\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$, providing a weights matrix $\mathbf{W}$ that should be a diagonal matrix with diagonal elements $w_{ii} = \dfrac{1}{\sigma^2_{i}}$.

# What does `mvreg` do?
The approach that `mvreg` follows is to fit a model in which expected value and variance are jointly modelled as functions of linear predictors, which can have different structures for the two components. 

Formally, the model is specified as $$Y_i|\mathbf{x}_i, \mathbf{z}_i \sim \mathcal{N}\left(\mu_i = \mathbf{x}_i'\boldsymbol{\beta}, \sigma^2_i = \exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}\right)$$
where $\mathbf{x}_i$ is the $i$-th row vector of a $n \times k$ design matrix $\mathbf{X}$ and $\boldsymbol{\beta}$ is the vector of parameters for the mean component, while $\mathbf{z}_i$ is the $i$-th row vector of a $n \times p$ design matrix $\mathbf{Z}$ and $\boldsymbol{\tau}$ is the vector of parameters for the variance component.

In general, $\mathbf{X}$ and $\mathbf{Z}$ can share the same columns, can have totally different columns or the columns of $\mathbf{Z}$ can be a subset of the columns of $\mathbf{X}$ and viceversa.

The variance is linked to the linear predictor $\mathbf{z}_i'\boldsymbol{\tau}$ via a $\log$-link, so that 

$$\log{(\sigma^2_i)} = \mathbf{z}_i'\boldsymbol{\tau}$$

Model parameters are estimated using the maximum likelihood method, but before delving into technical details let's see how `mvreg` works.

## `mvreg()` function, a `lm`-like function for heteroscedastic regression.

The main function that a generic user of `mvreg` should use is the `mvreg()` function. 
This function allows to fit a heroscedastic linear model in which two different formulas can be specified, one for mean component and the other one for variance component.

Let's see some examples of different case uses.

For the first example we'll use the `cars`. This dataset contains only two variables, `speed` and `dist`, so that it's natural to consider a model that explains how stopping distance is affected by speed. Let's visualize the data.

```{r, echo = F, warning = F, message = F}
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point(shape = 1) + 
  geom_smooth(linewidth = 0.5, method = "lm") +
  theme_few()
```

It is clearly evident that stopping distance increases as speed increases, and a keen eye might notice that for low values of speed the stopping distance tends to assume values closer to the regression line, while, as speed increases, the points tend to be more dispersed.

Let's fit a classic linear model and check the relationship between the square root of the absolute value of residuals and the fitted values.

```{r}
mod.lm <- lm(dist ~ speed, data = cars)
```

```{r, echo = F, warning = F, message = F}
# cars |>
#   mutate(residuals = scale(mod.lm$residuals), 
#          fitted.values = mod.lm$fitted.values) |> 
#   ggplot(aes(x = fitted.values, y = sqrt(abs(residuals)))) + 
#   geom_point(shape = 1) + 
#   geom_smooth(color = "red", linewidth = 0.5) + 
#   labs(y = expression(sqrt(abs(scaled~residuals)))) +
#   labs(x = "fitted values") + 
#   theme_few()

plot(mod.lm, 3)
```

The graph seems to confirm the initial evidence with respect to the possibility of observing heteroscedasticity, so we could try to account for it via `mvreg`.

`mvreg()` function works just like `lm()` function. We can specify two different formulas for the mean component, which is called `formula.mu`, and for the variance component, which is called `formula.s2`.

```{r}
mod.mvreg <- mvreg(formula.mu = dist ~ speed, 
                   formula.s2 = ~speed, data = cars)
```

`mvreg()` returns an object of class `mvreg`, for which various generic methods are implemented.
Let's see the `summary()` of the model.

```{r}
summary(mod.mvreg)
```
As it can be seen, the output of `summary()` returns two main tables. In the first table estimates, standard errors and the results of Wald tests are reported for the mean component, whereas the second table reports same information for the coefficient of the `log(variance)` component. These tables have the same structure of a classical output of `lm` objects, with the main difference that Wald tests are computed assuming asymptotic normal distribution under the null hypothesis that the coefficients are equal to zero.

It can be seen that the coefficient relative to the variance component for the variable `speed` is significantly different from zero and it's positive, suggesting a positive linear relation between the logarithm of the variance and speed.

## How to handle `formula.s2`
The argument `formula.s2` is optional. If it is not specified it is assumed that the structure of linear predictor is the same both in mean and variance component.

For example, if we write

```{r}
mvreg(dist ~ speed, data = cars)
```

we get the same model we fitted in the previous section.

If we do not want the formulas to be equal, it is necessary to specify both of them, as in 

```{r}
mvreg(dist ~ speed + I(speed^2),
      ~speed, data = cars)
```
`formula.s2` can be both a two-side formula or a only right-side formula. `mvreg()` essentially ignores the left side of `formula.s2`, as it is needed only to build the design matrix for variance component.


# Methods and applications
Let's look at the statistical methods behind `mvreg`.

## Model specification and loglikelihood

As before, the model is specified as

$$Y_i|\mathbf{x}_i, \mathbf{z}_i \sim \mathcal{N}\left(\mu_i = \mathbf{x}_i'\boldsymbol{\beta}, \sigma^2_i = \exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}\right)$$
with probability density function

$$f(y_i|\mathbf{x}_i, \mathbf{z}_i) = \dfrac{1}{\sqrt{2\pi\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}}\exp\left\{ -\dfrac{1}{2}\dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}} \right\}$$

and log-likelihood function


$$ \ell(\boldsymbol{\beta},\boldsymbol{\tau}) = -\dfrac{1}{2}\sum_{i=1}^n \left\{\log(2\pi) + \mathbf{z}_i'\boldsymbol{\tau} + \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}} \right\} $$

The expressions for the first and second derivatives of the log-likelihood are provided as follows:

$$\dfrac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \dfrac{y_i - \mathbf{x}_i'\boldsymbol{\beta}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}x_{ij}$$
$$\dfrac{\partial^2 \ell}{\partial \beta_j \partial \beta_l} = -\sum_{i=1}^n \dfrac{x_{ij}x_{il}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}$$

$$\dfrac{\partial \ell}{\partial \tau_j} = -\dfrac{1}{2}\sum_{i=1}^n\left\{ 1 - \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}z_{ij}\right\} $$

$$\dfrac{\partial^2 \ell}{\partial \tau_j \partial \tau_l} = -\dfrac{1}{2}\sum_{i=1}^n \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}z_{ij}z_{il} $$

$$\dfrac{\partial^2 \ell}{\partial \beta_j \partial \tau_j} = -\sum_{i=1}^n \dfrac{y_i - \mathbf{x}_i'\boldsymbol{\beta}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}x_{ij}z_{il}$$

With $\mathcal{U}_\boldsymbol{\beta}(\boldsymbol{\beta}, \boldsymbol{\tau})$ and $\mathcal{U}_\boldsymbol{\tau}(\boldsymbol{\beta}, \boldsymbol{\tau})$ we indicate the gradient column vectors with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ respectively.

With $\mathcal{H}_\boldsymbol{\beta}(\boldsymbol{\beta}, \boldsymbol{\tau})$ and $\mathcal{H}_\boldsymbol{\tau}(\boldsymbol{\beta}, \boldsymbol{\tau})$ we indicate the blocks of the hessian matrix with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ respectively.

Similarly, $\mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})$ is the block of cross derivatives with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$.

The full hessian is 

$$\mathcal{H}(\boldsymbol{\beta}, \boldsymbol{\tau}) = 
\begin{bmatrix}
    \mathcal{H}_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\tau}) & \mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau}) \\
    \mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}'(\boldsymbol{\beta}, \boldsymbol{\tau}) & \mathcal{H}_{\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})
\end{bmatrix}$$

$\mathcal{J}(\boldsymbol{\beta}, \boldsymbol{\tau}) = -\mathcal{H}(\boldsymbol{\beta}, \boldsymbol{\tau})$ is the observed Fisher information matrix.

The expected Fisher information matrix is defined as $\mathcal{I}(\boldsymbol{\beta}, \boldsymbol{\tau}) = \mathbb{E}(\mathcal{J}(\boldsymbol{\beta}, \boldsymbol{\tau}))$ and it is used in the computation of variance-covariance matrix of the estimates of parameters.

We have

$$\left\{\mathcal{I}_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\tau})\right\}_{jl} = \sum_{i=1}^n \dfrac{x_{ij}x_{il}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}$$



$$\left\{\mathcal{I}_{\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})\right\}_{jl} = \dfrac{1}{2}\sum_{i=1}^n z_{ij}z_{il}$$
$$\left\{\mathcal{I}_{\boldsymbol{\beta}\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})\right\}_{jl} = 0$$


## Parameters estimation
The vectors of parameters $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ are estimated via maximum likelihood.

A closed form expression for $\hat{\boldsymbol{\beta}}$ is the WLS estimator $\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$, in which $\mathbf{W}$ is a diagonal matrix with $w_{ij} = \dfrac{1}{\exp\left\{\mathbf{z}_i'\hat{\boldsymbol{\tau}}\right\}} \text{ } \forall i = j$.

There is not a closed form expression for $\hat{\boldsymbol{\tau}}$, so Newton-Raphson method is required to estimate it.

### Starting values
Starting values are provided by function `mvreg_start()`, which takes in input the vector of response variable `y`, the design matrices `x` and `z` and the specification of the method to get the first guess of $\hat{\boldsymbol{\tau}}_0$ via the argument `start.s2`.
A starting value of $\hat{\boldsymbol{\beta}}_0$ is computed via classic OLS estimation.
There are three different ways to get a initial guess of $\hat{\boldsymbol{\tau}}_0$.

The default method is provided by `start.s2 = "residuals"`, in which $\hat{\boldsymbol{\beta}}_0$ is used to estimate empirical residuals $\hat{\varepsilon}_i = y_i - \mathbf{x}_i'\hat{\boldsymbol{\beta}}_0$. Then the log of the squares of these residuals are regressed on $\mathbf{Z}$ to find a first guess of $\hat{\boldsymbol{\tau}}_0$. 

A second option is `start.s2 = "gamma"`, in which the squares of the empirical residuals are regressed on $\mathbf{Z}$ via a Gamma glm regression.

The last option is `start.s2 = "zero"`, in which a first guess is simply given by  $\hat{\boldsymbol{\tau}}_0 = (\log\hat{S}^2, 0, 0, \ldots, 0)_p'$.

### Estimation algorithm
The algorithm for fitting the parameters via maximum likelihood is contained in the `mvreg_fit()` function, which takes in input the vector of response variable `y`, the design matrices `x` and `z` and starting values `b0` and `t0` provided by `mvreg_start()`.

At the $k$-th iteration, the algorithm takes values of $\hat{\boldsymbol{\beta}}_{k-1}$ and $\hat{\boldsymbol{\tau}}_{k-1}$ and uses them to compute 

$$\hat{\boldsymbol{\tau}}_k = \hat{\boldsymbol{\tau}}_{k-1} - \mathcal{H}^{-1}_\boldsymbol{\tau}(\hat{\boldsymbol{\beta}}_{k-1}, \hat{\boldsymbol{\tau}}_{k-1}) \mathcal{U}_\boldsymbol{\tau}(\hat{\boldsymbol{\beta}}_{k-1}, \hat{\boldsymbol{\tau}}_{k-1})$$

Then weights $w_i$ are estimated via $w_i = \dfrac{1}{\exp\left\{\mathbf{z}_i'\hat{\boldsymbol{\tau}_{k}}\right\}}$ and used to to compute $$\hat{\boldsymbol{\beta}}_k = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$$

The algorithm continues until $|(\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\tau}})_k - (\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\tau}})_{k-1}| < \boldsymbol{\varepsilon}$

This algorithm is the default choice for `mvreg_fit()`, but an argument `method` is available. If `method = "wls"` the algorithm just described is used, else if `method = "full_nr"` the closed form expression for $\hat{\boldsymbol{\beta}}$ is not used and Newton method is used for parameters of both mean and variance components.




























