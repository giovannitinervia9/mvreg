---
title: "mvreg"
output:
  rmarkdown::html_document:
    highlight: pygments
    toc: true
vignette: >
  %\VignetteIndexEntry{mvreg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.width = 12 / 2.5, fig.height = 8 / 2.5)
```

```{r setup, include = F}
library(mvreg)
suppressMessages(require(dplyr))
suppressMessages(require(ggplot2))
suppressMessages(require(ggthemes))
```

# Why `mvreg`? The heteroscedasticity problem

`mvreg` is a package that allows users to fit linear models in which the assumption of homoscedasticity can be relaxed.

It is well known that in the standard linear regression model one of the assumptions is that the variance of the residuals is constant for all the observations. However, in real-word applications it is not uncommon to encounter cases where this assumption does not hold.

In such cases, the Ordinary Least Squares (OLS) estimator for the parameter vector $\boldsymbol{\beta}$ remains unbiased, but the standard errors produced by `lm()` are no longer consistent.

This problem can be addressed in several ways, such as using sandwich variance estimation, as provided by the `sandwich` package, or by employing Weighted Least Squares (WLS) estimation. In WLS, the $\boldsymbol{\beta}$ parameter is estimated as $\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$, where $\mathbf{W}$ is a diagonal weight matrix with diagonal elements $w_{i} = \dfrac{1}{\sigma^2_{i}}$.

# What does `mvreg` do?
The approach that `mvreg` follows is to fit a model in which expected value and variance are jointly modeled as functions of linear predictors, which can have different structures for the two components. 

Formally, the model is specified as $$Y_i|\mathbf{x}_i, \mathbf{z}_i \sim \mathcal{N}\left(\mu_i = \mathbf{x}_i'\boldsymbol{\beta}, \sigma^2_i = \exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}\right)$$
where $\mathbf{x}_i$ is the $i$-th row vector of a $n \times k$ design matrix $\mathbf{X}$ and $\boldsymbol{\beta}$ is the vector of parameters for the mean component, while $\mathbf{z}_i$ is the $i$-th row vector of a $n \times p$ design matrix $\mathbf{Z}$ and $\boldsymbol{\tau}$ is the vector of parameters for the variance component.

In general, $\mathbf{X}$ and $\mathbf{Z}$ can share the same columns, have totally different columns or the columns of $\mathbf{Z}$ can be a subset of the columns of $\mathbf{X}$ and vice versa.

The variance is linked to the linear predictor $\mathbf{z}_i'\boldsymbol{\tau}$ via a $\log$-link, so that 

$$\log{(\sigma^2_i)} = \mathbf{z}_i'\boldsymbol{\tau}$$

The model parameters are estimated using maximum likelihood method, but before delving into technical details let's see how `mvreg()` works.

## `mvreg()`: a function for fitting heteroscedastic linear models

The main function that a typical `mvreg` user will interact with is the `mvreg()` function.
This function fits a heteroscedastic linear model in which two different formulas can be specified, one for the mean component and another for the variance component.

Let’s consider the `cars` dataset, which contains two variables: `speed` and `dist`. It is natural to model how the stopping distance (`dist`) is influenced by `speed`. Let’s visualize the data.

```{r, echo = F, warning = F, message = F}
ggplot(cars, aes(x = speed, y = dist)) +
  geom_point(shape = 1) +
  geom_smooth(linewidth = 0.5, method = "lm") +
  theme_few()
```

It is clear that stopping distance increases with speed, and one might notice that, for lower speeds, stopping distances tend to assume values closer to the regression line, whereas at higher speeds, the points tend to be more dispersed.

Let's fit a classic linear model and check the relationship between the square root of the absolute value of residuals and the fitted values.

```{r}
mod.lm <- lm(dist ~ speed, data = cars)
```

```{r, echo = F, warning = F, message = F}
# cars |>
#   mutate(residuals = scale(mod.lm$residuals),
#          fitted.values = mod.lm$fitted.values) |>
#   ggplot(aes(x = fitted.values, y = sqrt(abs(residuals)))) +
#   geom_point(shape = 1) +
#   geom_smooth(color = "red", linewidth = 0.5) +
#   labs(y = expression(sqrt(abs(scaled~residuals)))) +
#   labs(x = "fitted values") +
#   theme_few()

plot(mod.lm, 3)
```

The graph seems to confirm initial suspicions about heteroscedasticity, so we can attempt to account for it using `mvreg`.

The `mvreg()` function operates similarly to the `lm()` function. We can specify two different formulas, one for the mean component, `formula.mu`, and another for the variance component, `formula.s2`.

```{r}
mod.mvreg <- mvreg(
  formula.mu = dist ~ speed,
  formula.s2 = ~speed, data = cars
)
```

`mvreg()` returns an object of class `mvreg`, for which various generic functions are available.
Let's examine the `summary()` of the model.

```{r}
summary(mod.mvreg)
```

As shown, the output of `summary()` returns two main tables, one for the `mean` component and another for the `log(variance)` component. Both tables include estimates, standard errors, and Wald test results, similar to the classical `lm` output. The key difference is that Wald tests assume an asymptotic normal distribution under the null hypothesis that the coefficients are zero.

The coefficient for the variance component related to `speed` is significantly different from zero and is positive, indicating a positive linear relationship between the logarithm of the variance and speed.

The `summary()` output also includes model fit statistics such as the log-likelihood (logLik), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC).

Additionally, a Likelihood Ratio Test (LRT) is performed to compare the `mvreg` model with a simpler `lm` model that assumes constant variance. In this example the test suggests that the `mvreg` model, which allows for heteroskedasticity, provides a significantly better fit than the `lm` model.


## How to handle `formula.s2`
The `formula.s2` argument is optional. If not provided, it is assumed that the linear predictor structure is the same for both the mean and variance components.

For example, this model

```{r}
mvreg(dist ~ speed, data = cars)
```

yields the same result as the model fitted in the previous section.

If we want different formulas for the mean and variance components, we must specify both, as shown here 

```{r}
mvreg(dist ~ speed + I(speed^2),
  ~speed,
  data = cars
)
```
The `formula.s2` can be a two-sided formula or a right-hand-side formula. `mvreg()` essentially ignores the left-hand side of `formula.s2`, as it is only needed to construct the design matrix for the variance component.


# Methods and applications
Let's look at the statistical methods underlying `mvreg`.

## Model specification and loglikelihood

As before, the model is specified as

$$Y_i|\mathbf{x}_i, \mathbf{z}_i \sim \mathcal{N}\left(\mu_i = \mathbf{x}_i'\boldsymbol{\beta}, \sigma^2_i = \exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}\right)$$
with probability density function

$$f(y_i|\mathbf{x}_i, \mathbf{z}_i) = \dfrac{1}{\sqrt{2\pi\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}}\exp\left\{ -\dfrac{1}{2}\dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}} \right\}$$

and log-likelihood function


$$ \ell(\boldsymbol{\beta},\boldsymbol{\tau}) = -\dfrac{1}{2}\sum_{i=1}^n \left\{\log(2\pi) + \mathbf{z}_i'\boldsymbol{\tau} + \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}} \right\} $$

The expressions for the first and second derivatives of the log-likelihood with respect to the parameters of the mean component are

$$\dfrac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n \dfrac{y_i - \mathbf{x}_i'\boldsymbol{\beta}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}x_{ij}$$


$$\dfrac{\partial^2 \ell}{\partial \beta_j \partial \beta_l} = -\sum_{i=1}^n \dfrac{x_{ij}x_{il}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}$$

$$\dfrac{\partial^2 \ell}{\partial \boldsymbol{\beta}  \partial \boldsymbol{\beta}'} = -\mathbf{X}' \mathbf{W}_{\boldsymbol{\beta}}\mathbf{X}$$


with $\mathbf{W}_{\boldsymbol{\beta}} = \text{diag}\left( \dfrac{1}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}\right)$

The expressions for the first and second derivatives of the log-likelihood with respect to the parameters of the variance component are

$$\dfrac{\partial \ell}{\partial \tau_j} = -\dfrac{1}{2}\sum_{i=1}^n\left\{1 - \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}\right
\}z_{ij} $$

$$\dfrac{\partial^2 \ell}{\partial \tau_j \partial \tau_l} = -\dfrac{1}{2}\sum_{i=1}^n \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}z_{ij}z_{il}$$
$$\dfrac{\partial^2 \ell}{\partial \boldsymbol{\tau} \partial \boldsymbol{\tau}'}  = -\dfrac{1}{2}\mathbf{Z}' \mathbf{W}_{\boldsymbol{\tau}} \mathbf{Z}$$


with $\mathbf{W}_{\boldsymbol{\tau}} = \text{diag}\left( \dfrac{\left(y_i - \mathbf{x}_i'\boldsymbol{\beta}\right)^2}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}\right)$

The expression for the cross derivatives of the log-likelihood with respect to the parameters of the mean and variance component is

$$\dfrac{\partial^2 \ell}{\partial \beta_j \partial \tau_j} = -\sum_{i=1}^n \dfrac{y_i - \mathbf{x}_i'\boldsymbol{\beta}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}x_{ij}z_{il}$$
$$\dfrac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\tau}'} = - \mathbf{X}'\mathbf{W}_{\boldsymbol{\beta \tau}}\mathbf{Z}$$

with $\mathbf{W}_{\boldsymbol{\beta \tau}} = \text{diag}\left( \dfrac{y_i - \mathbf{x}_i'\boldsymbol{\beta}}{\exp\left\{\mathbf{z}_i'\boldsymbol{\tau}\right\}}\right)$

With $\mathcal{U}_\boldsymbol{\beta}(\boldsymbol{\beta}, \boldsymbol{\tau})$ and $\mathcal{U}_\boldsymbol{\tau}(\boldsymbol{\beta}, \boldsymbol{\tau})$ we represent the gradient column vectors with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ respectively.

With $\mathcal{H}_\boldsymbol{\beta}(\boldsymbol{\beta}, \boldsymbol{\tau})$ and $\mathcal{H}_\boldsymbol{\tau}(\boldsymbol{\beta}, \boldsymbol{\tau})$ we indicate the blocks of the hessian matrix with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ respectively.

Similarly, $\mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})$ is the block of cross derivatives with respect to $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$.

The full hessian is 

$$\mathcal{H}(\boldsymbol{\beta}, \boldsymbol{\tau}) = 
\begin{bmatrix}
    \mathcal{H}_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\tau}) & \mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau}) \\
    \mathcal{H}_{\boldsymbol{\beta} \boldsymbol{\tau}}'(\boldsymbol{\beta}, \boldsymbol{\tau}) & \mathcal{H}_{\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau})
\end{bmatrix}$$

$\mathcal{J}(\boldsymbol{\beta}, \boldsymbol{\tau}) = -\mathcal{H}(\boldsymbol{\beta}, \boldsymbol{\tau})$ is the observed Fisher information matrix.

The expected Fisher information matrix is defined as $\mathcal{I}(\boldsymbol{\beta}, \boldsymbol{\tau}) = \mathbb{E}(\mathcal{J}(\boldsymbol{\beta}, \boldsymbol{\tau}))$ and it is used in the computation of variance-covariance matrix of the estimators of parameters.

We have
$$\mathcal{I}_{\boldsymbol{\beta}}(\boldsymbol{\beta}, \boldsymbol{\tau}) = \mathbf{X}' \mathbf{W}_{\boldsymbol{\beta}}\mathbf{X}$$



$$\mathcal{I}_{\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau}) = \dfrac{1}{2}\mathbf{Z}'\mathbf{Z}$$
$$\mathcal{I}_{\boldsymbol{\beta}\boldsymbol{\tau}}(\boldsymbol{\beta}, \boldsymbol{\tau}) = \mathbf{0}$$


## Parameters estimation
The parameters vector $\boldsymbol{\beta}$ and $\boldsymbol{\tau}$ are estimated via maximum likelihood.

A closed form expression for $\hat{\boldsymbol{\beta}}$ is the weighted least squares (WLS) estimator $\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$, in which $\mathbf{W}$ is a diagonal matrix with elements $w_{i} = \dfrac{1}{\exp\left\{\mathbf{z}_i'\hat{\boldsymbol{\tau}}\right\}}$.

There is no closed-form solution for $\hat{\boldsymbol{\tau}}$, so the Newton-Raphson method is used to estimate it.

### Starting values
Starting values for the parameters are provided by the function `mvreg_start()`, which takes as input the response vector `y`, the design matrices `x` and `z` and a method for generating the initial guess for $\hat{\boldsymbol{\tau}}_0$ via the argument `start.s2`. The argument `start.s2` can be specified in `mvreg()` function, which passes its value to `mvreg_start()`.

The initial value for $\hat{\boldsymbol{\beta}}_0$ is obtained through ordinary least squares (OLS) estimation. 

There are three methods available for obtaining an initial guess of $\hat{\boldsymbol{\tau}}_0$.

The default method is provided by `start.s2 = "residuals"`, in which $\hat{\boldsymbol{\beta}}_0$ is used to estimate empirical residuals $\hat{\varepsilon}_i = y_i - \mathbf{x}_i'\hat{\boldsymbol{\beta}}_0$. Then the log of the squares of these residuals are regressed on $\mathbf{Z}$ to find a first guess of $\hat{\boldsymbol{\tau}}_0$. 

A second option is `start.s2 = "gamma"`, in which the squares of the empirical residuals are regressed on $\mathbf{Z}$ using a Gamma generalized linear model.

The last and simpler option is `start.s2 = "zero"`, in which the first guess is simply given by $\hat{\boldsymbol{\tau}}_0 = (\log\hat{S}^2_y, 0, 0, \ldots, 0)_p'$.

### Estimation algorithm
The algorithm for fitting the parameters via maximum likelihood is contained in the `mvreg_fit()` function, which takes as input the response vector `y`, the design matrices `x` and `z` and the starting values `b0` and `t0` provided by `mvreg_start()`.

At the $k$-th iteration, the algorithm takes the current estimates $\hat{\boldsymbol{\beta}}_{k-1}$ and $\hat{\boldsymbol{\tau}}_{k-1}$ to compute 

$$\hat{\boldsymbol{\tau}}_k = \hat{\boldsymbol{\tau}}_{k-1} + \mathcal{I}^{-1}_\boldsymbol{\tau}(\hat{\boldsymbol{\beta}}_{k-1}, \hat{\boldsymbol{\tau}}_{k-1}) \mathcal{U}_\boldsymbol{\tau}(\hat{\boldsymbol{\beta}}_{k-1}, \hat{\boldsymbol{\tau}}_{k-1})$$

Next, the weights $w_{i}$ are estimated via $w_{i} = \dfrac{1}{\exp\left\{\mathbf{z}_i'\hat{\boldsymbol{\tau}_{k}}\right\}}$ and used to to compute $$\hat{\boldsymbol{\beta}}_k = (\mathbf{X}'\mathbf{W}\mathbf{X})^{-1}\mathbf{X}'\mathbf{W}\mathbf{y}$$

The algorithm continues iterating until the convergence criterion $|(\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\tau}})_k - (\hat{\boldsymbol{\beta}},\hat{\boldsymbol{\tau}})_{k-1}| < \boldsymbol{\varepsilon}$ is satisfied or the maximum number of iteration is reached (specified in the `maxit` argument). $\boldsymbol{\varepsilon}$ is a scalar positive value specified in the `tol` argument.

This algorithm is the default choice for `mvreg_fit()`, but an argument `method` is available. If `method = "wls"` the algorithm described above is used. If `method = "full_nr"` the closed form expression for $\hat{\boldsymbol{\beta}}$ is not used and the Newton-Raphson method is applied to both the mean and variance components. The argument `method` can be specified in `mvreg()` function, which passes its value to `mvreg_fit()`.

The `vcov.fit` argument is also available and allows the user to choose whether to use the `expected` (default) or the `observed` Fisher information matrix in the algorithm.

Once an `mvreg` model is fitted, parameter estimates can be accessed via the `coef()` method, which has a `partition` argument to specify whether to return all coefficients (default, `partition = "all"`), only the mean component (`partition = "mu"`), or only the variance component (`partition = "s2"`).
```{r}
coef(mod.mvreg)
coef(mod.mvreg, "mu")
coef(mod.mvreg, "s2")
```



## Variance-covariance matrix of estimators

According to asymptotic likelihood theory, we have that $$ (\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})' \rightarrow \mathcal{N}\left[ (\boldsymbol{\beta}, \boldsymbol{\tau})', \mathcal{I}^{-1}(\boldsymbol{\beta}, \boldsymbol{\tau})\right] $$

An estimate of the variance-covariance matrix is given by $\mathcal{I}^{-1}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})$

The variance-covariance matrix is computed by `mvreg_fit()` function, which has a `vcov.type` argument. By default `vcov.type = "expected"`, meaning that the variance-covariance matrix is computed using the expected Fisher information matrix $\mathcal{I}^{-1}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})$. Alternatively, the user can specify to use the observed Fisher information matrix $\mathcal{J}^{-1}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}})$ by specifying `vcov.type = "observed"`. The argument `vcov.type` can be specified in `mvreg()` function, which passes its value to `mvreg_fit()`.

Once a `mvreg` model is fitted, the variance-covariance matrix can be accessed using `vcov()` method.  
The `vcov` method for `mvreg` objects also has a `partition` argument, which allows the user to specify which diagonal block of the matrix should be returned. The default is `"all`", but `"mu"` (for the mean component) and `"s2"` (for the variance component) can also be selected.

```{r}
vcov(mod.mvreg)
vcov(mod.mvreg, "mu")
vcov(mod.mvreg, "s2")
```

## Confidence intervals
Once a `mvreg` model is fitted, it is possibile to obtain confidence intervals for both $\hat{\boldsymbol{\beta}}$ and $\hat{\boldsymbol{\tau}}$ using the generic function `confint()`.
This function takes the fitted `mvreg` model as input, along with an optional specification of the parameters (`parm`) for which the confidence intervals should be computed, and a confidence level (`level`), which defaults to 0.95.

The confidence intervals returned by `confint()` are classic Wald intervals of the form $$\hat{\theta} \pm z_{1 - \frac{\alpha}{2}} \cdot \text{SE}(\hat{\theta})$$

For example, to compute a 99% confidence interval for all parameters of the `mod.mvreg`:
```{r}
confint(mod.mvreg, level = 0.99)
```
If `parm` is not specified, confidence intervals for all parameters will be returned. Onecan also limit the output by specifying the `parm` argument, which accepts either a character vector with the parameter names or a numeric vector with the indices of the parameters. For instance:

```{r}
confint(mod.mvreg, parm = c(1, 3))
```

```{r}
confint(mod.mvreg, parm = c("mu.speed", "s2.speed"))
```




## Fitted values and predictions
Once $\hat{\boldsymbol{\beta}}$ and $\hat{\boldsymbol{\tau}}$ are estimated, one can compute an estimate for the mean, the log of the variance and variance components for the observed values of $\mathbf{x}_i'$ and $\mathbf{z}_i'$ as follows 

$$\hat{\mu}_i = \mathbf{x}_i'\hat{\boldsymbol{\beta}}$$ 

$$\log\hat{\sigma}^2_i = \mathbf{z}_i'\hat{\boldsymbol{\tau}}$$ 

$$\hat{\sigma}^2_i = \exp\left\{\mathbf{z}_i'\hat{\boldsymbol{\tau}}\right\}$$

These values can be returned by the generic function `fitted()`, which accepts the argument `type`. By default, `type = "all"` so all three quantities are estimated, but the user can specify only `"mu"`, `"log.s2"` or `"s2"` to estimate only one of the components.

```{r}
head(fitted(mod.mvreg))
```

The generic `predict()` is also available. It takes as argument the `mvreg` object, and if no additional parameters are specified, its output is the same as `fitted()`. However, the predictions are returned as a list with three different elements for `mu`, `log.s2` and `s2`.

```{r}
predict(mod.mvreg)
```

The `predict()` function also has a `type` parameter, which works the same as in `fitted()`, but allows multiple components to be returned simultaneously.

It is possible to pass a `newdata` argument, which must be a `data.frame` containing the same variables used for fitting the `mvreg` model, both for the mean and the variance components

```{r}
newdata <- data.frame(speed = as.vector(summary(cars$speed)))

predict(mod.mvreg, newdata = newdata)
```

A further argument is `se.fit`, a logical value indicating whether to return an estimate of the standard error for the predictions. The standard errors for `mu` and `log.s2` are computed using standard formulas

$$\text{SE}(\hat{\mu}_i) = \sqrt{ \mathbf{x}_i\mathcal{I}^{-1}_\boldsymbol{\beta}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}}) \mathbf{x}_i'}$$

$$\text{SE}(\log \hat{\sigma}_i^2) = \sqrt{ \mathbf{z}_i\mathcal{I}^{-1}_\boldsymbol{\tau}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}}) \mathbf{z}_i'}$$

while the standard error for $\hat{\sigma}^2_i$ is estimated using Delta method

$$\text{SE}( \hat{\sigma}_i^2) = \sqrt{\exp\left\{2\mathbf{z}_i'\hat{\boldsymbol{\tau}}\right\}\left(\text{SE}(\log \hat{\sigma}_i^2)\right)^2}$$


```{r}
predict(mod.mvreg, newdata = newdata, se.fit = T)
```

The argument `interval` is a logical value indicating whether to return a confidence interval for the predictions. The user can specify the confidence level by the `level` argument. The intervals for $\mu_i$ and $\log \sigma^2_i$ are computed using the classic Wald pivotal quantity

$$ \left\{\hat{\mu}_i + z_{\frac{\alpha}{2}}\text{SE}(\hat{\mu}_i) \leq \mu_i \leq \hat{\mu}_i + z_{1 -\frac{\alpha}{2}}\text{SE}(\hat{\mu}_i)\right\}$$


$$ \left\{\log\hat{\sigma}^2_i + z_{\frac{\alpha}{2}}\text{SE}(\log \hat{\sigma}^2_i) \leq \log \sigma^2_i \leq \log \hat{\sigma}^2_i + z_{1 -\frac{\alpha}{2}}\text{SE}(\log\hat{\sigma}^2_i)\right\}$$

The interval for $\sigma^2_i$ is obtained by applying the exponential function to the interval for $\log \sigma^2_i$.



```{r}
predict(mod.mvreg, newdata = newdata, se.fit = T, interval = T, level = 0.95)
```


## Nested model comparison

Nested model comparisons can be performed using the generic function `anova()`.

The `anova()` function accepts one or more models and behaves differently based on the input. 

If only one model is provided, the comparison is conducted separately for the parameters of the mean component and the variance component.

If two or more models are provided, they can be ranked either by the number of parameters  in increasing order (`order.models = T`) or in a user-defined sequence (`order.models = F`). Comparisons are made sequentially between each pair of models. It is important to note that the function includes a check using `are_models_nested()` to verify whether the models are indeed nested; if they are not, a warning is issued indicating that the likelihood ratio test (LRT) is not meaningful.

### `anova()` on a single model
For the mean component, the function fits all possible nested models, using the full `formula.s2` as specified in the input model. The comparison begins with the intercept-only model, incrementally adding variables one by one from left to right as they appear in the original `formula.mu`. For each comparison, the model with the first $k$ variables is compared to the model with the first $k + 1$ variables. These comparisons are performed using likelihood ratio tests of the form:

$$-2\log\Lambda = -2\log\dfrac{\mathcal{L}(\hat{\boldsymbol{\beta}}_{(k)}, \hat{\boldsymbol{\tau}})}{\mathcal{L}(\hat{\boldsymbol{\beta}}_{(k+1)}, \hat{\boldsymbol{\tau}})}$$ where $\hat{\boldsymbol{\beta}}_{(k)}$ is the vector of maximum likelihood estimates of $\boldsymbol{\beta}$ given that the `formula.mu` contains the first $k$ variables and $\hat{\boldsymbol{\beta}}_{(k+1)}$ is the vector of maximum likelihood estimates given that the `formula.mu` contains the first $k + 1$ variables. $\hat{\boldsymbol{\tau}}$ is the vector of maximum likelihood estimates of the parameters for the variance component, which is held constant from the full `formula.s2`.
According to standard asymptotic theory, the test statistic follows a chi-squared distribution $\chi^2_g$ where $g$ is the difference in the dimensionality of the parameter spaces being compared $g = \text{dim}(\hat{\boldsymbol{\beta}}_{(k+1)}) - \text{dim}(\hat{\boldsymbol{\beta}}_{(k)})$. 

The same algorithm is used for nested model comparison within `formula.s2`, but here the likelihood ratio test statistic is: $$-2\log\Lambda = -2\log\dfrac{\mathcal{L}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}}_{(k)})}{\mathcal{L}(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\tau}}_{(k+1)})}$$

Let's fit a `mvreg` model to the `iris` dataset and use `anova()` to compare models:

```{r}
mod.mvreg1 <- mvreg(Sepal.Length ~ Species + Sepal.Width, ~Species, data = iris)

anova(mod.mvreg1)
```

The `anova()` functions outputs two tables, one for the mean model comparison and the other one for the variance model comparison.

For the mean model comparison, the variance component is held constant as specified in the full model (`formula.s2`), and vice versa for the variance model comparison.

The tables report values for the negative two log-likelihood (`-2logLik`), the number of parameters (`n.param`), the Akaike Information Criterion (`AIC`), and the residual degrees of freedom (`df.residuals`) for each model.

The first row of each table doesn't report values for the likelihood ratio test statistic (`LRT`), degrees of freedom (`df`), or p-value (`Pr(>Chi)`), as no comparison is made. However, these statistics are shown in subsequent rows. For example, the second row compares `model2` to `model1`, showing that `model2` provides a significantly better fit. Similarly, `model3` is compared to `model2`, and again, a significant improvement in fit is evident.

The same interpretation applies to the variance model comparison, where `model2` (with variance modeled as a function of `Species`) is a significantly better fit than the intercept-only `model1`, assuming the mean component is estimated as specified in the full model.


### `anova()` with two or more models
When two or more models are passed to the `anova()` function, they are by default ranked in increasing order based on the number of parameters. In the case of ties, models are sorted in decreasing order of `-2logLik`. The function then proceeds to conduct sequential pairwise comparisons using a likelihood ratio test. If one prefers not to have the models ordered, it is possible to set the `order.models` argument to `FALSE`.

For example, if we want to compare a model in which the variance of `Sepal.Length` is modeled as a function of `Species` and `Sepal.Width` with a model in which it is only modeled as a function of `Species`, we can fit the two models and use `anova()` to compare them. We can use the `update()` function to remove `Sepal.Width` from the full model by specifying the new formula for the variance component in `new.formula.s2`.

```{r}
mod1 <- mvreg(Sepal.Length ~ Species + Sepal.Width, data = iris)
mod2 <- update(mod1, new.formula.s2 = ~Species)

anova(mod1, mod2)
```

The output of `anova()` when two or more models are passed consists of one table where the models are listed by row. Minus two log-likelihood (`-2logLik`), number of parameters (`n.param`), Akaike information criterion (`AIC`), and residual degrees of freedom (`df.residual`) are printed for all models. Additionally, the likelihood ratio test statistic (`LRT`), its degrees of freedom (`df`), and p-value (`Pr(>Chi)`) are printed in the rows where a model is compared to the one preceding it.

Information about the model formulas being compared is printed before the table.


### `anova()` with non-nested models

The likelihood ratio test (LRT) for comparing models is only valid for nested models. When multiple models are provided, the function checks for nesting using `are_models_nested()`. If any comparisons are made between non-nested models, a warning is issued, and the p-value is neither calculated nor printed.

For example, consider the following scenario where we fit two `mvreg` models and compare them using the `anova()` function:

```{r}
mod1 <- mvreg(Sepal.Length ~ Species, ~Species, data = iris)
mod2 <- mvreg(Sepal.Length ~ Species, ~Sepal.Width, data = iris)
anova(mod1, mod2)
```
In this case, `mod1` and `mod2` are not nested models, which is correctly identified by the `anova()` function. As a result, the p-value is not printed and a warning is raised. The function raises a warning instead of an error because non-nested models can still be compared based on their `AIC` values.
















































